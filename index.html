<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 17px;
        margin-left: auto;
        margin-right: auto;
        width: 980px;
    }

    h1 {
        font-weight: 300;
        line-height: 1.15em;
    }

    h2 {
        font-size: 1.75em;
    }

    a:link, a:visited {
        color: #B6486F;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    h1, h2, h3 {
        text-align: center;
    }

    h1 {
        font-size: 40px;
        font-weight: 500;
    }

    h2 {
        font-weight: 400;
        margin: 16px 0px 4px 0px;
    }

    .paper-title {
        padding: 16px 0px 16px 0px;
    }

    section {
        margin: 32px 0px 32px 0px;
        text-align: justify;
        clear: both;
    }

    .col-5 {
        width: 20%;
        float: left;
    }

    .col-4 {
        width: 25%;
        float: left;
    }

    .col-3 {
        width: 33%;
        float: left;
    }

    .col-2 {
        width: 50%;
        float: left;
    }

    .col-1 {
        width: 100%;
        float: left;
    }

    .row, .author-row, .affil-row {
        overflow: auto;
    }

    .author-row, .affil-row {
        font-size: 26px;
    }

    .row {
        margin: 16px 0px 16px 0px;
    }

    .authors {
        font-size: 26px;
    }

    .affil-row {
        margin-top: 16px;
    }

    .teaser {
        max-width: 100%;
    }

    .text-center {
        text-align: center;
    }

    .screenshot {
        width: 256px;
        border: 1px solid #ddd;
    }

    .screenshot-el {
        margin-bottom: 16px;
    }

    hr {
        height: 1px;
        border: 0;
        border-top: 1px solid #ddd;
        margin: 0;
    }

    .material-icons {
        vertical-align: -6px;
    }

    p {
        line-height: 1.25em;
    }

    .caption {
        font-size: 16px;
        /*font-style: italic;*/
        color: #666;
        text-align: center;
        margin-top: 4px;
        margin-bottom: 10px;
    }

    video {
        display: block;
        margin: auto;
    }

    figure {
        display: block;
        margin: auto;
        margin-top: 10px;
        margin-bottom: 10px;
    }

    #bibtex pre {
        font-size: 13.5px;
        background-color: #eee;
        padding: 16px;
    }

    .blue {
        color: #2c82c9;
        font-weight: bold;
    }

    .orange {
        color: #d35400;
        font-weight: bold;
    }

    .flex-row {
        display: flex;
        flex-flow: row wrap;
        justify-content: space-around;
        padding: 0;
        margin: 0;
        list-style: none;
    }

    .paper-btn {
        position: relative;
        text-align: center;

        display: inline-block;
        margin: 8px;
        padding: 8px 8px;

        border-width: 0;
        outline: none;
        border-radius: 2px;

        background-color: #B6486F;
        color: white !important;
        font-size: 20px;
        width: 100px;
        font-weight: 600;
    }

    .paper-btn-parent {
        display: flex;
        justify-content: center;
        margin: 16px 0px;
    }

    .paper-btn:hover {
        opacity: 0.85;
    }

    .container {
        margin-left: auto;
        margin-right: auto;
        padding-left: 16px;
        padding-right: 16px;
    }

    .venue {
        /*color: #B6486F;*/
        font-size: 30px;

    }

</style>

<!-- End : Google Analytics Code-->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
      rel='stylesheet' type='text/css'>
<head>
    
    <title>SynthDetX: Exploring Deep Learning for Synthetic Image Generation and Detection - Course Hackathon.</title>
    <meta property="og:description" content="SynthDetX: Exploring Deep Learning for Synthetic Image Generation and Detection - Course Hackathon"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
    <meta name="twitter:card" content="summary_large_image">
</head>

<body>
<div class="container">
    <div class="paper-title">
        <h1>SynthDetX: Exploring Deep Learning for Synthetic Image Generation and Detection - Course Hackathon</h1>
    </div>
    <center><img src="./assets/008.gif" alt="Gif Image"></center>
    <div id="authors">
        <center>
            <center>
                <table align=center width=500px>
                    <tr>
                        <td align=center width=200px>
                            <center>
                                <span style="font-size:20px"><sup>1</sup> NVIDIA</span>
                            </center>
                        </td>
                        <td align=center width=200px>
                            <center>
                                <span style="font-size:20px"><sup>2</sup> Caltech</span>
                            </center>
                        </td>
                    </tr>
                </table>
            </center>

        </center>
        <br>
        <!-- <center><img width="20%" src="./assets/nvidialogo.png" style="margin-top: 20px; margin-bottom: 3px;"></center> -->
        <!--        <div class="affil-row">-->
        <!--            <div class="venue text-center"><b>ICLR 2022 (spotlight)</b></div>-->
        <!--        </div>-->
        <br>
        <div style="clear: both">
            <div class="paper-btn-parent">
                <a class="paper-btn" href="https://arxiv.org/">
                    <span class="material-icons"> description </span>
                    Paper
                </a>
                <a class="paper-btn" href="https://github.com/">
                    <span class="material-icons"> code </span>
                    Code
                </a>
            </div>
        </div>
    </div>

    <section id="abstract"/>
    <h2>Abstract</h2>
    <hr>
    <div class="flex-row">
        <p>
            <span style="font-size: 20px;">
            <b>During this Hackathon, participants will be divided into two teams with distinct responsibilities. 
                One team will primarily focus on the generation of synthetic images,
                 while the other team will concentrate on the detection of these synthetic images. 
                 The ultimate objective behind this division of labor is to contribute to the betterment of 
                 humanity.

The widespread adoption and extensive usage of stable diffusion models on one hand, 
and the user-friendly nature of trained models on the other, have led to their significant popularity in
 various media applications. 
 <br><br>
 However, this popularity raises an important question: How can one determine whether an image is synthetic or genuine?

This challenge of discerning between synthetic and real images is a significant aspect that will be 
addressed during the Hackathon. Participants will explore innovative approaches, leveraging the 
power of deep learning and advanced algorithms, to develop detection methods capable of 
accurately distinguishing between synthetic and authentic images. <br><br>By tackling this question head-on, 
the Hackathon aims to contribute to the advancement of image analysis techniques and provide valuable 
insights into the reliability and trustworthiness of media content. </b></span></p>
        </p>
    </div>
    </section>


    <section id="teaser-image">
        </p>
        <figure style="margin-top: 20px; margin-bottom: 20px;">
            <center><img width="50%" src="./assets/001.png" style="margin-bottom: 20px;"></center>
            <p class="caption">
                A sample high-quality image generated by a recent SD model named epiCRealism.
                The generative prompt is: <br><br>
                RAW Photo, DSLR BREAK
(kkw-ph1:0.9) BREAK
half body portrait of a young 20yo woman, black hair, wearing a summer dress BREAK
detailed, blowing bubble gum, green bubble gum <lora:Bubble Gum:0.65>, professional colorgraded 
                <a  href="https://civitai.com/images/1306596?modelVersionId=105035&prioritizedUserIds=81744&period=AllTime&sort=Most+Reactions&limit=20">
                    Link
                </a>
            </p>
            <p class="caption">
            </p>
        </figure>
    </section>

    <section id="results">
        <h2>Introduction and general approach:</h2>
        <hr>
        <div class="flex-row">
            <p>
            <b>Our primary objective is to engage in the generation of synthetic images and subsequently 
            undertake the task of identifying them</b>. However, it is crucial to acknowledge that this 
            process is more intricate than it initially appears. A key consideration is determining 
            which Stable Diffusion (SD) models to employ, as there exists a plethora of models, 
            and it remains uncertain whether they all possess common characteristics.
            <br><br> 
            Moreover, selecting appropriate post-processing methods presents another challenge. 
            For instance, exploring the potential of compressing and subsequently decompressing 
            a synthetic image to enhance its realism and make it more challenging to identify is
            an intriguing avenue to explore. Additionally, incorporating noise into the synthetic 
            images is another avenue that warrants investigation. Furthermore, the impact of 
            adversarial attacks on the generated synthetic images is an important aspect that 
            must be taken into account.
            <br><br>
            All of these questions remain open and require careful consideration. 
            We envision the development of a black box system that leverages a 
            pipeline incorporating the aforementioned techniques to generate synthetic images. 
            This comprehensive approach will enable us to explore the boundaries of synthetic 
            image generation and detection, paving the way for advancements in this field. 
        </p>   
        </div>
       

    </section>

   
    <section id="SD"/>
    <h2>SD models used for sysntehic image generarion (all of them should be downloaded)</h2>
    <hr>
    <!-- <div class="flex-row"> -->
        At the bare minimum, all these models should be readily available:
        <b><ol>
            <li>epicrealism_pureEvolutionV3.safetensors</li>
            <li>amIReal_V2.safetensors</li>
            <li>Put Stable Diffusion checkpoints here.txt</li>
            <li>instruct-pix2pix-00-22000.safetensors</li>
            <li>dreamlike-photoreal-2.0.ckpt</li>
            <li>v2-1_512-ema-pruned.ckpt</li>
            <li>sd-v1-4-full-ema.ckpt</li>
            <li>sd-v1-4.ckpt</li>
          </ol> </b>
        <p>
            <b>Name: PureEvolution, Size: 2GB, <a href="https://civitai.com/api/download/models/105035"> Download link</a></b><BR>
            This is a highly realistic model for the generation of portraits. 
            <br><b>Sample prompt:</b><br> 
            High detail RAW color, (full body Photo) of an unattractive man, wide hips, wearing a ((dirty grey hoodie, sweatpants)), ((scruffy beard, balding, chubby face, wide chin, squinting, glasses)), (standing in front of a wall of monitors and computers in an office), realistic, symmetrical, highly detailed, harsh lighting, cinematic lighting, serious eyes, contrast, textured skin, cold skin pores, hasselblad, 45 degree, hard light, gigapixel, pimples, 85mm, F/4
            <center><img width="50%" src="./assets/002.png" style="margin-bottom: 20px;"></center>    
 
        </p>
        <p>
            <b>Name: AmIreal, Size: 2GB, Download link:.</b><BR>
            This is a highly realistic model
        </p>

    <section id="SD"/>
    <h2>SD extensions (all of them should be downloaded AND installed)</h2>
    <video autoplay>
        <source src="assets/travel-00000.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    SD models offer powerful extensions that significantly enhance their capabilities. 
    One such notable extension is the <b>"seed travel"</b> extension, which enables the perturbation of an image,
     resulting in the batch creation of numerous variations of the same image. 
     This extension amplifies the versatility and potential of SD models,
      allowing for the exploration of a wide range of image transformations 
      and generating diverse sets of images based on a single source image. 
      This feature opens up exciting possibilities for creativity, experimentation, 
      and generating large datasets with rich variations, ultimately pushing the boundaries of 
      synthetic image generation and expanding the scope of applications for SD models. 

      <figure style="width: 100%">
        <center><img width="100%" src="assets/007.png"></center>
        <p class="caption" style="margin-bottom: 24px;">
            Installed SD Extenstions.
        </p>
    </figure>

      <figure style="width: 100%">
        <center><img width="45%" src="assets/006.png"></center>
        <p class="caption" style="margin-bottom: 24px;">
            SD Extenstions folder.
        </p>
    </figure>
    <hr>
        <p>
            <b>Name: Seed travel <a href="https://github.com/yownas/seed_travel"> Download link</a></b><BR>
            Batch generation of images using text2img etc. 
            <br><b>Sample:</b><br> 
              <div>
                <video id="my-video" muted>
                  <source src="assets/travel-00000.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <button onclick="playVideo()">Play</button>
              </div>
              
              <script>
                var video = document.getElementById("my-video");
              
                function playVideo() {
                  video.play();
                }
              </script>
              
          
 
        </p>
        <p>
            <b>Name: AmIreal, Size: 2GB, Download link:.</b><BR>
            This is a highly realistic model
        </p>
    </section>


    <section id="paper">
        <h2>Papers & references</h2>
        <hr>
        <div class="flex-row">
            <!-- <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href=""><img class="screenshot" src="assets/diffusion_purification_preview.jpg"></a>
            </div> -->
            <div style="width: 50%; font-size: 20px;">
                <p><b>Diffusion Models for Adversarial Purification</b></p>
        
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2205.07460"> arXiv
                    version</a></div>
               
            </div>
        </div>
    </section>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{nie2022DiffPure,
  title={Diffusion Models for Adversarial Purification},
  author={Nie, Weili and Guo, Brandon and Huang, Yujia and Xiao, Chaowei and Vahdat, Arash and Anandkumar, Anima},
  booktitle = {International Conference on Machine Learning (ICML)},
  year={2022}
}</code></pre>
    </section>
</div>
</body>
</html>

